{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\App\\vscode1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set save directory\n",
    "save_path = \"D:/App/vscode1/news\"\n",
    "os.makedirs(save_path, exist_ok=True)  # Ensure directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = [item[\"text\"] for item in dataset[\"train\"]]\n",
    "labels = [item[\"label\"] for item in dataset[\"train\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Sampling for Small Dataset\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "for train_idx, test_idx in splitter.split(texts, labels):\n",
    "    small_train_texts = [texts[i] for i in train_idx[:10000]]\n",
    "    small_train_labels = [labels[i] for i in train_idx[:10000]]\n",
    "    small_test_texts = [texts[i] for i in test_idx[:5000]]\n",
    "    small_test_labels = [labels[i] for i in test_idx[:5000]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"prajjwal1/bert-tiny\"  # A small and fast alternative\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(small_train_labels)\n",
    "y_test = label_encoder.transform(small_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Inputs\n",
    "def tokenize(texts, tokenizer, max_length=512):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "train_encodings = tokenize(small_train_texts, tokenizer)\n",
    "test_encodings = tokenize(small_test_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch Dataset\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}, self.labels[idx]\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, y_train)\n",
    "test_dataset = NewsDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1438976067323654\n",
      "Model saved for epoch 1\n",
      "Epoch 2, Loss: 0.6783305663651171\n",
      "Model saved for epoch 2\n",
      "Epoch 3, Loss: 0.46302506365715124\n",
      "Model saved for epoch 3\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch, labels in train_loader:\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "        torch.save(model.state_dict(), f\"D:\\\\App\\\\vscode1\\\\news\\\\bert_tiny_news_classifier_epoch{epoch+1}.pth\")\n",
    "        print(f\"Model saved for epoch {epoch+1}\")\n",
    "\n",
    "# Train the Model\n",
    "train_model(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyBERT model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save Final Model\n",
    "torch.save(model.state_dict(), \"D:\\\\App\\\\vscode1\\\\news\\\\bert_tiny_news_classifier_final.pth\")\n",
    "print(\"TinyBERT model trained and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89      1267\n",
      "           1       0.95      0.97      0.96      1240\n",
      "           2       0.85      0.80      0.83      1252\n",
      "           3       0.82      0.90      0.86      1241\n",
      "\n",
      "    accuracy                           0.88      5000\n",
      "   macro avg       0.88      0.88      0.88      5000\n",
      "weighted avg       0.88      0.88      0.88      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate Model\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch, labels in test_loader:\n",
    "    batch = {key: val.to(device) for key, val in batch.items()}\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch).logits\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print Classification Report\n",
    "print(classification_report(true_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Input: Stock markets are experiencing a huge crash this week.\n",
      "🔹 Predicted Class: Business\n",
      "\n",
      "📌 Input: The Mars rover has discovered signs of ancient life.\n",
      "🔹 Predicted Class: Science/Tech\n",
      "\n",
      "📌 Input: Argentina wins FIFA World Cup 2026 after an intense match!\n",
      "🔹 Predicted Class: Sports\n",
      "\n",
      "📌 Input: New AI technology is revolutionizing the tech industry.\n",
      "🔹 Predicted Class: Science/Tech\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Define Label Mapping\n",
    "label_mapping = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Science/Tech\"\n",
    "}\n",
    "\n",
    "#  Prediction Function\n",
    "def predict_label(model, tokenizer, text):\n",
    "    model.eval()  # Ensure evaluation mode\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs).logits\n",
    "        pred_label = torch.argmax(output, dim=1).cpu().item()\n",
    "    \n",
    "    return label_mapping[pred_label]  # Map numeric label to category\n",
    "\n",
    "#  Example Usage\n",
    "sample_texts = [\n",
    "    \"Stock markets are experiencing a huge crash this week.\",\n",
    "    \"The Mars rover has discovered signs of ancient life.\",\n",
    "    \"Argentina wins FIFA World Cup 2026 after an intense match!\",\n",
    "    \"New AI technology is revolutionizing the tech industry.\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    predicted_label = predict_label(model, tokenizer, text)\n",
    "    print(f'📌 Input: {text}\\n🔹 Predicted Class: {predicted_label}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
